{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1KYwcuQBmhA4OIkM3z9LJ5CyWMRcN7gCd",
      "authorship_tag": "ABX9TyPGJdbiBlAxcoukQQq/rlzF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanhHung2112/Indeed_crawler/blob/main/Skills_extract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "EERxpSpUO1YW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c44abf0-8158-4fdc-e564-e90e2e697b3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-16 15:51:34.079312: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 15:51:34.079380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 15:51:34.081346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 15:51:35.487562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "import json"
      ],
      "metadata": {
        "id": "ErgLdMl8GKCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained spaCy model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add the NER component to the pipeline\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Load training data from JSON file\n",
        "def load_data_from_json(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            data = json.load(file)\n",
        "        return data\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Try loading the data\n",
        "training_data = load_data_from_json(\"/content/job_skill_ner.json\")\n",
        "\n",
        "# Check if data is loaded successfully\n",
        "if training_data is not None:\n",
        "    print(f\"Loaded {len(training_data)} examples from the JSON file.\")\n",
        "else:\n",
        "    print(\"Failed to load data. Please check the JSON file and try again.\")\n",
        "\n",
        "# Rest of your code (combining data, converting to spaCy format, training the model, etc.)\n",
        "\n",
        "\n",
        "# # Convert combined data to spaCy format\n",
        "combined_examples = []\n",
        "for entry in combined_data:\n",
        "    text = entry[\"text\"]\n",
        "    entities = entry[\"entities\"]\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, {\"entities\": entities})\n",
        "    combined_examples.append(example)\n",
        "\n",
        "# Train the NER model with the combined data\n",
        "nlp.begin_training()\n",
        "for _ in range(10):  # Adjust the number of epochs\n",
        "    for example in combined_examples:\n",
        "        nlp.update([example], drop=0.5)  # Adjust dropout\n",
        "\n",
        "# Save the trained model\n",
        "nlp.to_disk(\"job_skills_ner_model\")\n"
      ],
      "metadata": {
        "id": "b9dlCrA7yuY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings (optional, use with caution)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"spacy\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"tensorflow\")\n",
        "\n",
        "# Load training data from JSON file\n",
        "def load_data_from_json(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            data = json.load(file)\n",
        "        return data\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add the NER component to the pipeline\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Add the labels (in this case, just \"SKILL\")\n",
        "ner.add_label(\"SKILL\")\n",
        "\n",
        "# Disable other pipeline components for training efficiency\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "\n",
        "# Specify the path to your JSON file\n",
        "json_file_path = \"/content/skill.json\"\n",
        "\n",
        "# Load training data from JSON file\n",
        "training_data = load_data_from_json(json_file_path)\n",
        "\n",
        "if training_data:\n",
        "    # Remove duplicate entities based on start and end positions\n",
        "    cleaned_training_data = []\n",
        "    seen_entities = set()\n",
        "\n",
        "    for entry in training_data:\n",
        "        labels_list = entry.get(\"label\", [])\n",
        "\n",
        "        # Ensure that labels is a list of dictionaries\n",
        "        if isinstance(labels_list, str):\n",
        "            labels_list = [{\"text\": labels_list, \"start\": -1, \"end\": -1, \"labels\": [\"SKILL\"]}]\n",
        "        elif not isinstance(labels_list, list):\n",
        "            continue  # Skip entries without a valid label list\n",
        "\n",
        "        cleaned_labels_list = []\n",
        "        for label in labels_list:\n",
        "            start = label.get(\"start\", -1)\n",
        "            end = label.get(\"end\", -1)\n",
        "            label_text = label.get(\"labels\", \"\")\n",
        "\n",
        "            # Ensure unique start and end positions\n",
        "            if (start, end) not in seen_entities:\n",
        "                cleaned_labels_list.append({\"text\": label_text, \"start\": start, \"end\": end, \"labels\": [\"SKILL\"]})\n",
        "                seen_entities.add((start, end))\n",
        "\n",
        "        if cleaned_labels_list:\n",
        "            cleaned_training_data.append({\"text\": entry.get(\"text\", \"\"), \"label\": cleaned_labels_list})\n",
        "\n",
        "    with nlp.disable_pipes(*other_pipes):\n",
        "        # Initialize the training\n",
        "        nlp.begin_training()\n",
        "\n",
        "        # Iterate through the cleaned training data for multiple epochs\n",
        "        for epoch in range(25):\n",
        "            # Shuffle the cleaned training data\n",
        "            random.shuffle(cleaned_training_data)\n",
        "\n",
        "            # Create batches of cleaned training data\n",
        "            for entry in cleaned_training_data:\n",
        "                text = entry.get(\"text\", \"\")\n",
        "                labels_list = entry.get(\"label\", [])\n",
        "\n",
        "                # Process annotations and extract entities\n",
        "                entities = []\n",
        "                for label in labels_list:\n",
        "                    start = label.get(\"start\", -1)\n",
        "                    end = label.get(\"end\", -1)\n",
        "                    label_text = label.get(\"labels\", \"\")\n",
        "                    entities.append((start, end, label_text))\n",
        "\n",
        "                # Create a spaCy Example\n",
        "                example = Example.from_dict(nlp.make_doc(text), {\"entities\": entities})\n",
        "\n",
        "                # Update the model with iterating each example\n",
        "                nlp.update([example], drop=0.5)\n",
        "\n",
        "    # Save the trained model to disk /content/drive/MyDrive/NER/\n",
        "    nlp.to_disk(\"/content/drive/MyDrive/NER/skills_ner_model·\")\n",
        "else:\n",
        "    print(\"Failed to load training data from the JSON file.\")\n"
      ],
      "metadata": {
        "id": "nHArGO-JyZkv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Load the saved model from disk\n",
        "loaded_nlp = spacy.load(\"/content/drive/MyDrive/NER/skills_ner_model\")\n",
        "\n",
        "# Test the loaded model with some example texts\n",
        "test_texts = [\n",
        "    \"I am skilled in Python and Java programming.\",\n",
        "    \"My experience includes using TensorFlow for machine learning.\",\n",
        "    \"I have hands-on experience with MongoDB and MySQL.\",\n",
        "    \"Backend development with Node.js and Django is my expertise.\",\n",
        "    \"Build machine learning\",\n",
        "    \"\"\"\n",
        "    As Technical Account Manager you will support our internal and external clients in online marketing campaigns configuration. We are working on our own proprietary solution that is the main point of client’s Marketing Technology Stack, integrating data from various services and delivering this data to external providers.\n",
        "\n",
        "Tasks\n",
        "\n",
        "Analyzing client websites to make sure they have the data needed for online marketing campaigns.\n",
        "Setting up, running, checking, and turning off various services for clients.\n",
        "Solving issues with these services.\n",
        "Working with our developer teams to handle reported problems, configure services, improve existing solutions, and brainstorm new ones.\n",
        "Checking problem reports and passing them to the right teams with initial analysis results.\n",
        "Looking at services from external vendors that we haven't used yet when we need them.\n",
        "Completing tasks with clear documentation for reference in the future.\n",
        "Opportunity to become the go-to person for a particular area of our knowledge base and manage internal team documentation.\n",
        "Actively participating in team meetings where we all discuss team matters (Governance Meeting).\n",
        "Requirements\n",
        "\n",
        "You should know about internet browsers (understand what happens when you enter a website URL and hit \"Enter\").\n",
        "You should be comfortable using Developer tools in browsers (able to analyze network traffic and inspect data layer - like JavaScript variables and DOM elements variables).\n",
        "You should be familiar with Online Marketing tools, such as:\n",
        "Principles of Tag management systems (like Google Tag Manager, which is a must, and others like Tealium or Commanders Act)\n",
        "Analytics tools like Google Analytics\n",
        "Online Marketing platforms, like Google Ads, Facebook Advertising, and others (Criteo, Google Marketing Platform, Microsoft Advertising, etc.)\n",
        "Consent Management Platforms (CMP) like Usercentrics, Klaro, OneTrust, Didomi\n",
        "You should have some knowledge of SQL (to write basic queries for database data retrieval).\n",
        "You should have some understanding of JavaScript (or a willingness to learn) to handle monitoring code implementation (tracking data, processing, using variables from DOM or URL).\n",
        "You should be knowledgeable about online privacy topics, especially regarding tracking codes and Privacy Enhanced Technologies in various browsers.\n",
        "Fluency in English is a must.\n",
        "You should be able to document your work so that you or anyone else can pick up where you left off and retain valuable information for future reference (knowledge capture).\n",
        "Understanding processes related to IT service management (like ITIL) is a plus.\n",
        "You should be a good team player, adhering to mutually agreed-upon rules and be able to communicate on technical topics with both technical and non-technical individuals.\n",
        "Knowledge of the German language is an added advantage.\n",
        "You'll help maintain the team and company's positive image.\n",
        "You should understand and support the team's vision, to which you will be contributing.\n",
        "Benefits\n",
        "\n",
        "Work in a global team.\n",
        "remote work first company.\n",
        "Competitive pay.\n",
        "Enjoy perks like a MultiSport card and private health coverage.\n",
        "Grow professionally with learning opportunities.\n",
        "Join a friendly, energetic team with a laid-back work environment\n",
        "    \"\"\",\n",
        "    \"Client should have strong problem solving skills, and good at both reading and speaking English\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    doc = loaded_nlp(text)\n",
        "    print(\"Input Text:\", text)\n",
        "    print(\"Entities:\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55B-gHqDm0xC",
        "outputId": "08ae5e0e-38b5-4cbe-f9d3-ccf27d2d8a95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: I am skilled in Python and Java programming.\n",
            "Entities: [('Python', \"['SKILL']\"), ('Java programming', \"['SKILL']\")]\n",
            "\n",
            "Input Text: My experience includes using TensorFlow for machine learning.\n",
            "Entities: [('machine learning.', \"['SKILL']\")]\n",
            "\n",
            "Input Text: I have hands-on experience with MongoDB and MySQL.\n",
            "Entities: [('MySQL', \"['SKILL']\")]\n",
            "\n",
            "Input Text: Backend development with Node.js and Django is my expertise.\n",
            "Entities: []\n",
            "\n",
            "Input Text: Build machine learning\n",
            "Entities: [('machine learning', \"['SKILL']\")]\n",
            "\n",
            "Input Text: \n",
            "    As Technical Account Manager you will support our internal and external clients in online marketing campaigns configuration. We are working on our own proprietary solution that is the main point of client’s Marketing Technology Stack, integrating data from various services and delivering this data to external providers.\n",
            "\n",
            "Tasks\n",
            "\n",
            "Analyzing client websites to make sure they have the data needed for online marketing campaigns.\n",
            "Setting up, running, checking, and turning off various services for clients.\n",
            "Solving issues with these services.\n",
            "Working with our developer teams to handle reported problems, configure services, improve existing solutions, and brainstorm new ones.\n",
            "Checking problem reports and passing them to the right teams with initial analysis results.\n",
            "Looking at services from external vendors that we haven't used yet when we need them.\n",
            "Completing tasks with clear documentation for reference in the future.\n",
            "Opportunity to become the go-to person for a particular area of our knowledge base and manage internal team documentation.\n",
            "Actively participating in team meetings where we all discuss team matters (Governance Meeting).\n",
            "Requirements\n",
            "\n",
            "You should know about internet browsers (understand what happens when you enter a website URL and hit \"Enter\").\n",
            "You should be comfortable using Developer tools in browsers (able to analyze network traffic and inspect data layer - like JavaScript variables and DOM elements variables).\n",
            "You should be familiar with Online Marketing tools, such as:\n",
            "Principles of Tag management systems (like Google Tag Manager, which is a must, and others like Tealium or Commanders Act)\n",
            "Analytics tools like Google Analytics\n",
            "Online Marketing platforms, like Google Ads, Facebook Advertising, and others (Criteo, Google Marketing Platform, Microsoft Advertising, etc.)\n",
            "Consent Management Platforms (CMP) like Usercentrics, Klaro, OneTrust, Didomi\n",
            "You should have some knowledge of SQL (to write basic queries for database data retrieval).\n",
            "You should have some understanding of JavaScript (or a willingness to learn) to handle monitoring code implementation (tracking data, processing, using variables from DOM or URL).\n",
            "You should be knowledgeable about online privacy topics, especially regarding tracking codes and Privacy Enhanced Technologies in various browsers.\n",
            "Fluency in English is a must.\n",
            "You should be able to document your work so that you or anyone else can pick up where you left off and retain valuable information for future reference (knowledge capture).\n",
            "Understanding processes related to IT service management (like ITIL) is a plus.\n",
            "You should be a good team player, adhering to mutually agreed-upon rules and be able to communicate on technical topics with both technical and non-technical individuals.\n",
            "Knowledge of the German language is an added advantage.\n",
            "You'll help maintain the team and company's positive image.\n",
            "You should understand and support the team's vision, to which you will be contributing.\n",
            "Benefits\n",
            "\n",
            "Work in a global team.\n",
            "remote work first company.\n",
            "Competitive pay.\n",
            "Enjoy perks like a MultiSport card and private health coverage.\n",
            "Grow professionally with learning opportunities.\n",
            "Join a friendly, energetic team with a laid-back work environment\n",
            "    \n",
            "Entities: [('Account Manager', \"['SKILL']\"), ('Tasks', \"['SKILL']\"), ('Analyzing client', \"['SKILL']\"), ('Completing tasks', \"['SKILL']\"), ('Developer tools', \"['SKILL']\"), ('Principles', \"['SKILL']\"), ('Tag management', \"['SKILL']\"), ('Google Tag Manager', \"['SKILL']\"), ('Analytics tools', \"['SKILL']\"), ('Google Analytics', \"['SKILL']\"), ('Online Marketing platforms', \"['SKILL']\"), ('Google Ads', \"['SKILL']\"), ('Facebook Advertising', \"['SKILL']\"), ('Criteo', \"['SKILL']\"), ('Google Marketing Platform', \"['SKILL']\"), ('Microsoft Advertising', \"['SKILL']\"), ('Consent Management Platforms', \"['SKILL']\"), ('CMP', \"['SKILL']\"), ('Usercentrics', \"['SKILL']\"), ('Klaro', \"['SKILL']\"), ('SQL', \"['SKILL']\"), ('JavaScript', \"['SKILL']\"), ('can pick', \"['SKILL']\"), ('team player', \"['SKILL']\")]\n",
            "\n",
            "Input Text: Client should have strong problem solving skills, and good at both reading and speaking English\n",
            "Entities: [('speaking English', \"['SKILL']\")]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JptOl5mGBxnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}